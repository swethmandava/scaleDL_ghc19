{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Proof-of-concept to Production: How to scale your Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NCF is a simple DNN based model for recommendation.\n",
    "\n",
    "<img src=\"img/ncf_diagram.png\" width=\"600\" title=\"Neural Collaborative Filtering Model Overview\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "$$ min_{x \\in R^n} f(x) := \\frac{1}{M} \\sum_{i=1}^M f_i (x)$$\n",
    "\n",
    "Problem of SGD can be expressed as equation above where $ f_i $ is a loss function for data points $ i \\in \\{1,2...M\\} $ and x is the vector of weights being optimized. Stochastic Gradient Descent is often used to iteratively optimize the above function as shown below.\n",
    "\n",
    "$$ x_{k+1} = x_k - \\alpha_k \\frac{1}{|B_k|} \\sum_{i \\in B_k} \\delta f_i(x_k) $$\n",
    "\n",
    "where $B_k \\in \\{1,2...M\\} $ is a batch sampled from the dataset and $\\alpha_k$ is the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Setup\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "import neumf\n",
    "import math\n",
    "\n",
    "# Process Data\n",
    "\n",
    "def process_data():\n",
    "    processed_data = utils.process_data()\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "train_label, train_users, train_items, test_users, test_items, \\\n",
    "    dup_mask, real_indices, all_test_users, \\\n",
    "    nb_users, nb_items, mat = process_data()\n",
    "\n",
    "print('Load data done. #user=%d, #item=%d, #train=%d, #test=%d'\n",
    "      % (nb_users, nb_items, len(train_users),\n",
    "         nb_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NCF Model\n",
    "model = neumf.initialize_model(nb_users, nb_items)\n",
    "\n",
    "#Initialize SGD Optimizer\n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Opt Learning rate 0.0050\n",
      "Input Batch Size 4096\n",
      "\n",
      "Epoch = 0\n",
      "Train Throughput = 736327.9173\n",
      "Epoch 0: HR@10 = 0.8325, NDCG@10 = 0.5457, train_time = 134.87, val_time = 0.58\n",
      "New best hr!\n",
      "\n",
      "Epoch = 1\n",
      "Train Throughput = 921067.1092\n",
      "Epoch 1: HR@10 = 0.8331, NDCG@10 = 0.5460, train_time = 107.82, val_time = 0.58\n",
      "New best hr!\n",
      "\n",
      "Epoch = 2\n",
      "Train Throughput = 739430.0484\n",
      "Epoch 2: HR@10 = 0.8345, NDCG@10 = 0.5468, train_time = 134.30, val_time = 0.58\n",
      "New best hr!\n",
      "\n",
      "Epoch = 3\n",
      "Train Throughput = 918804.9649\n",
      "Epoch 3: HR@10 = 0.8392, NDCG@10 = 0.5490, train_time = 108.08, val_time = 0.58\n",
      "New best hr!\n",
      "\n",
      "Epoch = 4\n",
      "Train Throughput = 737332.3955\n",
      "Epoch 4: HR@10 = 0.8485, NDCG@10 = 0.5554, train_time = 134.69, val_time = 0.57\n",
      "New best hr!\n",
      "\n",
      "Epoch = 5\n",
      "Train Throughput = 735914.3612\n",
      "Epoch 5: HR@10 = 0.8632, NDCG@10 = 0.5724, train_time = 134.95, val_time = 0.57\n",
      "New best hr!\n",
      "\n",
      "Epoch = 6\n",
      "Train Throughput = 737593.7288\n",
      "Epoch 6: HR@10 = 0.8830, NDCG@10 = 0.5979, train_time = 134.64, val_time = 0.58\n",
      "New best hr!\n",
      "\n",
      "Epoch = 7\n",
      "Train Throughput = 883664.7350\n",
      "Epoch 7: HR@10 = 0.8894, NDCG@10 = 0.6152, train_time = 112.38, val_time = 0.57\n",
      "New best hr!\n",
      "\n",
      "Epoch = 8\n",
      "Train Throughput = 1064974.8944\n",
      "Epoch 8: HR@10 = 0.8974, NDCG@10 = 0.6265, train_time = 93.25, val_time = 0.57\n",
      "New best hr!\n",
      "\n",
      "Epoch = 9\n",
      "Train Throughput = 791079.2391\n",
      "Epoch 9: HR@10 = 0.9004, NDCG@10 = 0.6323, train_time = 125.54, val_time = 0.57\n",
      "New best hr!\n",
      "\n",
      "Best train throughput 1064974.8944\n",
      "Best accuracy 0.9004\n",
      "Time to Target 1226.2721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9003848569963825"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Takes ~20 minutes to run. In the interest of time, output is saved.\n",
    "#Run Training\n",
    "# batch_size = 4096\n",
    "# utils.train(model, optimizer, processed_data, batch_size, learning_rate, \n",
    "#             mode=\"train\", warmup_fn=None, scale_loss_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIZE MATTERS\n",
    "\n",
    "$$ x_{k+1} = x_k - \\alpha_k \\frac{1}{|B_k|} \\sum_{i \\in B_k} \\delta f_i(x_k) $$\n",
    "\n",
    "Increasing the size of a single batch improves parallelism, for example, by using GPU for computationally intensive subroutines like matrix multiplications or by using multiple cores to perform SGD in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096 * 16\n",
    "utils.train(model, optimizer, processed_data, batch_size, learning_rate, mode=\"perf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when trained with large batch sizes, accuracy drops as high as 5% were noted even for small networks due to loss in generalization. \n",
    "\n",
    "<img src=\"img/resnet_bs_error.png\" width=\"600\" title=\"ImageNet top-1 validation error vs minibatch size\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096 * 16\n",
    "\n",
    "model = neumf.initialize_model(nb_users, nb_items)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "utils.train(model, optimizer, processed_data, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence and Scaling Efficiency: Linear Scaling Rule\n",
    "\n",
    "* [1] Linear Scaling Rule - When the minibatch size is multiplied by k, multiply the learning rate by k.\n",
    "\n",
    "* It is straight forward to understand, for the same number of epochs by increasing batch size by k, k fewer steps are taken. Hence increasing the step size by k seems intuitive.\n",
    "\n",
    "<img src=\"img/lr_scaling.png\" width=\"1500\" title=\"Visual Analogy of Linear Scaling\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Batch Size Arbitrarily by 16x\n",
    "\n",
    "batch_size = 4096 * 16\n",
    "momentum = 0.9\n",
    "learning_rate = # FILL ME\n",
    "\n",
    "model = neumf.initialize_model(nb_users, nb_items)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "utils.train(model, optimizer, processed_data, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear memory and reload data if running into OOM Errors\n",
    "# %reset -f\n",
    "# train_label, train_users, train_items, test_users, test_items, \\\n",
    "#     dup_mask, real_indices, all_test_users, \\\n",
    "#     nb_users, nb_items, mat = process_data()\n",
    "\n",
    "# Select Batch size by 192x as it fits on our GPU\n",
    "\n",
    "batch_size = 4096 * 192\n",
    "momentum = 0.9\n",
    "learning_rate = # FILL ME\n",
    "\n",
    "model = neumf.initialize_model(nb_users, nb_items)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "utils.train(model, optimizer, processed_data, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence and Scaling Efficiency: Warmup\n",
    "\n",
    "* For even larger minibatches, Linear scaling rule is shown to break down when the network changes rapidly due to instability. When $\\alpha_k$ is large, the update $\\alpha_k |\\delta f_i(x_k)| $ can be larger than $x_k$ causing divergence. This causes the training to be highly dependent on the weight initialization and initial LR. We can use warmup to combat this.\n",
    "\n",
    "* Warmup is a method by which we gradually increase learning rate at the start of training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#code to perform warmup\n",
    "warmup_epochs = 0.1\n",
    "\n",
    "def warmup(optimizer, iter_i, batches_per_epoch):\n",
    "    warmup_iters = warmup_epochs * batches_per_epoch\n",
    "    if iter_i >= warmup_iters:\n",
    "        lr_current = learning_rate\n",
    "    else:\n",
    "        warmup_factor = math.exp(math.log(0.01) * (warmup_iters - iter_i) / warmup_iters)\n",
    "        lr_current = learning_rate * warmup_factor\n",
    "    for grp in optimizer.param_groups:\n",
    "        grp['lr'] = lr_current\n",
    "    return\n",
    "\n",
    "batch_size = 4096 * 192\n",
    "learning_rate = 0.005 * 192\n",
    "momentum = 0.9\n",
    "model = neumf.initialize_model(nb_users, nb_items)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "\n",
    "utils.train(model, optimizer, processed_data, batch_size, learning_rate, warmup_fn=warmup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence and Scaling Efficiency: LARS\n",
    "\n",
    "* Layer wise Adaptive Rate Scaling (LARS) - is another effective method popular to combat the instability caused by high learning rates. \n",
    "\n",
    "* Standard SGD has the same learning rate for all layers. when $\\lambda$ is large, the update might be larger that the weight itself causing divergence. LARS introduces another term $\\lambda$ for each layer l and trust coeficcient $\\eta < 1$. \n",
    "\n",
    "$$ x_{k+1}^{l} = x_k - \\alpha_k \\lambda^l \\delta f_i(x_k^l) $$\n",
    "\n",
    "$$ \\lambda^l = \\eta \\frac{||x_k^l||}{\\delta f_i(x_k^l)} $$\n",
    "\n",
    "\n",
    "<img src=\"img/lars.png\" width=\"600\" title=\"LARS: Alexnet-BN with B=8k\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lars\n",
    "\n",
    "model = neumf.initialize_model(nb_users, nb_items)\n",
    "optimizer = lars.LARS(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "\n",
    "utils.train(model, optimizer, processed_data, batch_size, learning_rate, warmup_fn=warmup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Efficiency: Mixed Precision Training\n",
    "\n",
    "Train with half precision while maintaining the network accuracy achieved with single precision resulting in\n",
    "* **Increased throughput**\n",
    "* **Increased Capability** due to reduced memory footprint enabling larger batch sizes/models\n",
    "\n",
    "<img src=\"img/mp_training.png\" width=\"600\" title=\"Mixed Precision Speedups\">\n",
    "\n",
    "In SSD, for example, 31% of gradient values become 0s are they are not representable in fp16 causing the model to diverge. How do we overcome this limitation? Loss scaling!\n",
    "\n",
    "<img src=\"img/fp16_gradients.png\" width=\"600\" title=\"Histogram of activation gradient magnitudes throughout FP32 training of Multibox SSD network\">\n",
    "\n",
    "Enabling mixed precision involves two steps: \n",
    "* Porting the model to use the half-precision data type where appropriate\n",
    "* loss scaling to preserve small gradient values.\n",
    "\n",
    "**As easy as adding three lines of code in PyTorch, TensorFlow and MXNet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096 * 192\n",
    "learning_rate = 0.005 * 192\n",
    "warmup_epochs = 0.1\n",
    "momentum = 0.9\n",
    "\n",
    "model = neumf.initialize_model(nb_users, nb_items)\n",
    "optimizer = lars.LARS(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=0.0001)\n",
    "\n",
    "import apex.amp as amp\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O2\")\n",
    "\n",
    "def scale_loss(optimizer, loss):\n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    return\n",
    "utils.train(model, optimizer, processed_data, batch_size, learning_rate, \n",
    "            warmup_fn=warmup, scale_loss_fn=scale_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What else can we do?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### References\n",
    "\n",
    "\\[1\\] https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Recommendation/NCF\n",
    "\n",
    "\\[2\\] Micikevicius, S. Narang, J. Alben, G. F. Diamos,E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, and H. Wu. Mixed precision training. CoRR, abs/1710.03740, 2017\n",
    "\n",
    "\\[3\\] Yang You, Igor Gitman, Boris Ginsburg. Large Batch Training of Convolutional Networks. arXiv:1708.03888\n",
    "\n",
    "\\[4\\] Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, Quoc V. Le. Don't Decay the Learning Rate, Increase the Batch Size. arXiv:1711.00489\n",
    "\n",
    "\\[5\\] Priya Goyal, Piotr Doll√°r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, angqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet n 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n",
    "\n",
    "\\[6\\] https://github.com/noahgolmant/pytorch-lars"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
